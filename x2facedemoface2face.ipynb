{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "x2facedemoface2face.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak9250/X2Face/blob/master/x2facedemoface2face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilyNVBCmQxKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install http://download.pytorch.org/whl/cu80/torch-0.2.0.post3-cp27-cp27mu-manylinux1_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "831hmV4dw-Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget url to your source image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7kZ_enQ4NGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install autocrop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLQkZfjU4Th9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!autocrop -i /content/path to source image/ -o /content/cropped/ -w 256 -H 256 --facePercent 70"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKf0Nc69xKlp",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1dHM9_9CO8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchvision==0.2.0 --no-deps --no-cache-dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3LUD_JPQWrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/oawiles/X2Face.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFVFPkKcQfYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd X2Face/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNCYq4RgQlCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd UnwrapMosaic/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPk-qYMcQo0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "from UnwrappedFace import UnwrappedFaceWeightedAverage, UnwrappedFaceWeightedAveragePose\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor, Compose, Scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0vU4iLDQ2OJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_batch(source_images, pose_images, requires_grad=False, volatile=False):\n",
        "    return model(pose_images, *source_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfYhx0m-j_zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
        "pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
        "\n",
        "def torch_load_model(path):\n",
        "    if sys.version_info[0] < 3:\n",
        "        if torch.cuda.is_available():\n",
        "            state_dict = torch.load(path)\n",
        "        else:\n",
        "            state_dict = torch.load(path, map_location='cpu')\n",
        "    else:\n",
        "        if torch.cuda.is_available():\n",
        "            state_dict = torch.load(path, pickle_module=pickle)\n",
        "        else:\n",
        "            state_dict = torch.load(path, map_location='cpu', pickle_module=pickle)\n",
        "    return state_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfbUFAm9RVl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/release_x2face_eccv.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Ys9m2ERcg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip release_x2face_eccv.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQOWDrE0kCQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_MODEL = 'release_models/' # Change to your path\n",
        "state_dict = torch.load(BASE_MODEL + 'x2face_model.pth')\n",
        "\n",
        "model = UnwrappedFaceWeightedAverage(output_num_channels=2, input_num_channels=3, inner_nc=128)\n",
        "model.load_state_dict(state_dict['state_dict'])\n",
        "model = model.cuda()\n",
        "\n",
        "model = model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSw8xjH5Rxb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "driver_path = '/content/X2Face/UnwrapMosaic/examples/Taylor_Swift/1.6/nuBaabkzzzI/'\n",
        "source_path = '/content/cropped/'\n",
        "\n",
        "driver_imgs = [driver_path + d for d in sorted(os.listdir(driver_path))][0:8] # 8 driving frames\n",
        "source_imgs  = [source_path + d for d in sorted(os.listdir(source_path))][0:1] # 1 source frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgW7YQmoZs__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_img(file_path):\n",
        "    img = Image.open(file_path)\n",
        "    transform = Compose([Scale((256,256)), ToTensor()])\n",
        "    return Variable(transform(img)).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCRyqSa9_ntB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Driving the source image with the driving sequence\n",
        "source_images = []\n",
        "for img in source_imgs:\n",
        "    source_images.append(load_img(img).unsqueeze(0).repeat(len(driver_imgs), 1, 1, 1))\n",
        "    \n",
        "driver_images = None\n",
        "for img in driver_imgs:\n",
        "    if driver_images is None:\n",
        "        driver_images = load_img(img).unsqueeze(0)\n",
        "    else:\n",
        "        driver_images = torch.cat((driver_images, load_img(img).unsqueeze(0)), 0)\n",
        "\n",
        "# Run the model for each\n",
        "result = run_batch(source_images, driver_images)\n",
        "result = result.clamp(min=0, max=1)\n",
        "img = torchvision.utils.make_grid(result.cpu().data)\n",
        "    \n",
        "# Visualise the results\n",
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 24.\n",
        "fig_size[1] = 24.\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "plt.axis('off')\n",
        "plt.savefig(\"test.png\")\n",
        "\n",
        "result_images = img.permute(1,2,0).numpy()\n",
        "driving_images = torchvision.utils.make_grid(driver_images.cpu().data).permute(1,2,0).numpy()\n",
        "print(\"The results is: \")\n",
        "plt.imshow(np.vstack((result_images, driving_images)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}